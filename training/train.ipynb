{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Imports",
   "id": "d1beea7b0d1b4aa6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T16:03:32.184169Z",
     "start_time": "2025-08-02T16:03:32.181031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sampling.conditional_probability_path import GaussianConditionalProbabilityPath\n",
    "from sampling.sampleable import PixelArtSampler\n",
    "from sampling.noise_scheduling import LinearAlpha, LinearBeta\n",
    "from models.unet import PixelArtUNet\n",
    "from training.trainer import UnguidedTrainer\n",
    "from diff_eq.ode_sde import UnguidedVectorFieldODE\n",
    "from diff_eq.simulator import EulerSimulator\n",
    "from evaluation import FID, InceptionV3FeatureExtractor\n",
    "\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cpu' #torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Train",
   "id": "15d6274bd2eaabd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T16:03:34.361118Z",
     "start_time": "2025-08-02T16:03:33.742059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 'unet' in locals():\n",
    "    del unet\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# Initialize probability path\n",
    "sampler = PixelArtSampler('../dataset/images').to(device)\n",
    "path = GaussianConditionalProbabilityPath(\n",
    "    p_data=sampler,\n",
    "    p_simple_shape = [4, 128, 128],\n",
    "    alpha = LinearAlpha(),\n",
    "    beta = LinearBeta()\n",
    ").to(device)\n",
    "\n",
    "# Initialize model\n",
    "unet = PixelArtUNet(\n",
    "    channels = [32, 64, 128],\n",
    "    num_residual_layers = 2,\n",
    "    t_embed_dim = 40\n",
    ")\n",
    "\n",
    "# Initialize evaluation metric\n",
    "inception_extractor = InceptionV3FeatureExtractor()\n",
    "metric = FID(\n",
    "    inception_extractor=InceptionV3FeatureExtractor(),\n",
    "    extractor_input_img_size=(299, 299)\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = UnguidedTrainer(\n",
    "    path=path,\n",
    "    model=unet,\n",
    "    experiment_dir=\"experiments/unet\",\n",
    "    eval_metric=metric\n",
    ")\n",
    "\n",
    "# Train :D\n",
    "trainer.train(\n",
    "    device=device,\n",
    "    num_epochs = 3,\n",
    "    batch_size=1,\n",
    "    lr=1e-3,\n",
    "    validate_every=1,\n",
    "    resume=False,\n",
    "    lr_warmup_steps_frac=0.1,\n",
    "    num_images_to_save=5,\n",
    "    save_images_every=1\n",
    ")"
   ],
   "id": "70527300bad06081",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# Initialize evaluation metric\u001B[39;00m\n\u001B[32m     23\u001B[39m inception_extractor = InceptionV3FeatureExtractor()\n\u001B[32m     24\u001B[39m metric = FID(\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m     inception_extractor=\u001B[43mInceptionV3FeatureExtractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m     26\u001B[39m     extractor_input_img_size=(\u001B[32m299\u001B[39m, \u001B[32m299\u001B[39m)\n\u001B[32m     27\u001B[39m )\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Initialize trainer\u001B[39;00m\n\u001B[32m     30\u001B[39m trainer = UnguidedTrainer(\n\u001B[32m     31\u001B[39m     path=path,\n\u001B[32m     32\u001B[39m     model=unet,\n\u001B[32m     33\u001B[39m     experiment_dir=\u001B[33m\"\u001B[39m\u001B[33mexperiments/unet\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     34\u001B[39m     eval_metric=metric\n\u001B[32m     35\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/training/evaluation.py:43\u001B[39m, in \u001B[36mInceptionV3FeatureExtractor.__init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     42\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m()\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m     inception = \u001B[43minception_v3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mInception_V3_Weights\u001B[49m\u001B[43m.\u001B[49m\u001B[43mIMAGENET1K_V1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform_input\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     44\u001B[39m     \u001B[38;5;28mself\u001B[39m.features = nn.Sequential(*\u001B[38;5;28mlist\u001B[39m(inception.children())[:-\u001B[32m1\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:142\u001B[39m, in \u001B[36mkwonly_to_pos_or_kw.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    135\u001B[39m     warnings.warn(\n\u001B[32m    136\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msequence_to_str(\u001B[38;5;28mtuple\u001B[39m(keyword_only_kwargs.keys()),\u001B[38;5;250m \u001B[39mseparate_last=\u001B[33m'\u001B[39m\u001B[33mand \u001B[39m\u001B[33m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m as positional \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    137\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    138\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33minstead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    139\u001B[39m     )\n\u001B[32m    140\u001B[39m     kwargs.update(keyword_only_kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:228\u001B[39m, in \u001B[36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    225\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m kwargs[pretrained_param]\n\u001B[32m    226\u001B[39m     kwargs[weights_param] = default_weights_arg\n\u001B[32m--> \u001B[39m\u001B[32m228\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbuilder\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torchvision/models/inception.py:473\u001B[39m, in \u001B[36minception_v3\u001B[39m\u001B[34m(weights, progress, **kwargs)\u001B[39m\n\u001B[32m    470\u001B[39m model = Inception3(**kwargs)\n\u001B[32m    472\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m weights \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m473\u001B[39m     model.load_state_dict(\u001B[43mweights\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprogress\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[32m    474\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m original_aux_logits:\n\u001B[32m    475\u001B[39m         model.aux_logits = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torchvision/models/_api.py:90\u001B[39m, in \u001B[36mWeightsEnum.get_state_dict\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     89\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_state_dict\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any) -> Mapping[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_state_dict_from_url\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torch/hub.py:875\u001B[39m, in \u001B[36mload_state_dict_from_url\u001B[39m\u001B[34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001B[39m\n\u001B[32m    873\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _is_legacy_zip_format(cached_file):\n\u001B[32m    874\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n\u001B[32m--> \u001B[39m\u001B[32m875\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcached_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torch/serialization.py:1525\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1523\u001B[39m             \u001B[38;5;28;01mexcept\u001B[39;00m pickle.UnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1524\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle.UnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1525\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1526\u001B[39m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1527\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1528\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1529\u001B[39m \u001B[43m            \u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[43m=\u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1530\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1531\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1532\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mmap:\n\u001B[32m   1533\u001B[39m     f_name = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torch/serialization.py:2114\u001B[39m, in \u001B[36m_load\u001B[39m\u001B[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001B[39m\n\u001B[32m   2112\u001B[39m \u001B[38;5;28;01mglobal\u001B[39;00m _serialization_tls\n\u001B[32m   2113\u001B[39m _serialization_tls.map_location = map_location\n\u001B[32m-> \u001B[39m\u001B[32m2114\u001B[39m result = \u001B[43munpickler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2115\u001B[39m _serialization_tls.map_location = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2117\u001B[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torch/serialization.py:2078\u001B[39m, in \u001B[36m_load.<locals>.persistent_load\u001B[39m\u001B[34m(saved_id)\u001B[39m\n\u001B[32m   2076\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2077\u001B[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001B[32m-> \u001B[39m\u001B[32m2078\u001B[39m     typed_storage = \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2079\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2080\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2082\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m typed_storage\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/pixel-art-character-diffusion/.venv/lib/python3.13/site-packages/torch/serialization.py:2031\u001B[39m, in \u001B[36m_load.<locals>.load_tensor\u001B[39m\u001B[34m(dtype, numel, key, location)\u001B[39m\n\u001B[32m   2024\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m storage_offset != zip_file.get_record_offset(name):\n\u001B[32m   2025\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   2026\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mThis is a debug assert that was run as the `TORCH_SERIALIZATION_DEBUG` environment \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2027\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mvariable was set: Incorrect offset for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstorage_offset\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m expected \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2028\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_file.get_record_offset(name)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   2029\u001B[39m             )\n\u001B[32m   2030\u001B[39m     storage = (\n\u001B[32m-> \u001B[39m\u001B[32m2031\u001B[39m         \u001B[43mzip_file\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_storage_from_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2032\u001B[39m         ._typed_storage()\n\u001B[32m   2033\u001B[39m         ._untyped_storage\n\u001B[32m   2034\u001B[39m     )\n\u001B[32m   2035\u001B[39m \u001B[38;5;66;03m# swap here if byteswapping is needed\u001B[39;00m\n\u001B[32m   2036\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m byteorderdata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Visualize results",
   "id": "c33ff8230cbdf0f8"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-31T17:24:18.027616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "num_samples = 10\n",
    "num_timesteps = 100\n",
    "\n",
    "# Simulate\n",
    "ts = torch.linspace(0, 1, num_timesteps).view(1, -1, 1, 1, 1).expand(num_samples, -1, 1, 1, 1).to(device)\n",
    "x0 = path.p_simple.sample(num_samples).to(device)  # (num_samples, 4, 128, 128)\n",
    "\n",
    "# Run simulation\n",
    "ode = UnguidedVectorFieldODE(unet)\n",
    "simulator = EulerSimulator(ode)\n",
    "x1 = simulator.simulate(x0, ts)  # (num_samples, 4, 128, 128)\n",
    "\n",
    "# Make grid from output (only use first 3 channels if RGB)\n",
    "img = x1[:, :3]  # (num_samples, 3, H, W)\n",
    "grid = make_grid(img, nrow=num_samples, normalize=True, value_range=(-1, 1))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "45ae061031325f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "4\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
